#!/usr/bin/env python3
"""
æµ‹è¯•æœºå™¨å­¦ä¹ ç®—æ³•è®­ç»ƒåŠŸèƒ½
éªŒè¯15ç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•çš„è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '.')

from quantanalyzer.model.trainer import ModelTrainer
from quantanalyzer.data.loader import DataLoader

def create_sample_data():
    """åˆ›å»ºæ ·æœ¬æ•°æ®ç”¨äºæµ‹è¯•"""
    # ç”Ÿæˆæ—¶é—´åºåˆ—
    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
    
    # åˆ›å»ºæ ·æœ¬æ•°æ®
    data = []
    for i, date in enumerate(dates):
        # åŸºç¡€ä»·æ ¼
        base_price = 100 + i * 0.1
        
        # ç”Ÿæˆç‰¹å¾
        feature1 = np.sin(i * 0.1) + np.random.normal(0, 0.1)
        feature2 = np.cos(i * 0.05) + np.random.normal(0, 0.1)
        feature3 = np.random.normal(0, 1)
        feature4 = np.random.uniform(0, 1)
        feature5 = np.log(i + 1) + np.random.normal(0, 0.1)
        
        # ç›®æ ‡å˜é‡ï¼ˆæ”¶ç›Šç‡ï¼‰
        target = feature1 * 0.3 + feature2 * 0.2 + feature3 * 0.1 + np.random.normal(0, 0.05)
        
        data.append({
            'datetime': date,
            'symbol': 'TEST',
            'open': base_price,
            'high': base_price + np.random.uniform(0, 2),
            'low': base_price - np.random.uniform(0, 2),
            'close': base_price + target,
            'volume': np.random.randint(1000, 10000),
            'feature1': feature1,
            'feature2': feature2,
            'feature3': feature3,
            'feature4': feature4,
            'feature5': feature5,
            'target': target
        })
    
    df = pd.DataFrame(data)
    df.set_index(['datetime', 'symbol'], inplace=True)
    return df

def test_ml_algorithms():
    """æµ‹è¯•æ‰€æœ‰æœºå™¨å­¦ä¹ ç®—æ³•"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æœºå™¨å­¦ä¹ ç®—æ³•è®­ç»ƒåŠŸèƒ½")
    print("=" * 60)
    
    # åˆ›å»ºæ ·æœ¬æ•°æ®
    print("ğŸ“Š åˆ›å»ºæ ·æœ¬æ•°æ®...")
    sample_data = create_sample_data()
    print(f"æ ·æœ¬æ•°æ®å½¢çŠ¶: {sample_data.shape}")
    
    # å®šä¹‰è¦æµ‹è¯•çš„ç®—æ³•
    algorithms = [
        'linear', 'ridge', 'lasso', 'elasticnet', 'logistic',
        'lightgbm', 'xgboost', 'random_forest', 'gradient_boosting', 'decision_tree', 'catboost',
        'svm', 'svr', 'naive_bayes', 'knn'
    ]
    
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
    feature_cols = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']
    X = sample_data[feature_cols]
    y = sample_data['target']
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    split_idx = int(len(X) * 0.7)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
    print()
    
    results = []
    
    for algo in algorithms:
        print(f"ğŸ§ª æµ‹è¯•ç®—æ³•: {algo}")
        
        try:
            # åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨
            trainer = ModelTrainer(model_type=algo)
            
            # è®­ç»ƒæ¨¡å‹
            print(f"  ğŸ“ˆ è®­ç»ƒæ¨¡å‹...")
            model = trainer.train(X_train, y_train)
            
            # é¢„æµ‹
            print(f"  ğŸ”® è¿›è¡Œé¢„æµ‹...")
            y_pred = trainer.predict(X_test)
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            print(f"  ğŸ“Š è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
            feature_importance = trainer.feature_importance
            
            # è¯„ä¼°æ¨¡å‹
            mse = np.mean((y_test - y_pred) ** 2)
            mae = np.mean(np.abs(y_test - y_pred))
            r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)
            
            # æ£€æŸ¥ç‰¹å¾é‡è¦æ€§
            if feature_importance is not None:
                importance_sum = feature_importance.sum()
                has_importance = importance_sum > 0
            else:
                has_importance = False
            
            results.append({
                'algorithm': algo,
                'status': 'âœ… æˆåŠŸ',
                'mse': mse,
                'mae': mae,
                'r2': r2,
                'has_feature_importance': has_importance,
                'error': None
            })
            
            print(f"  âœ… {algo} - è®­ç»ƒæˆåŠŸ")
            print(f"      MSE: {mse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}")
            print(f"      ç‰¹å¾é‡è¦æ€§: {'âœ… æœ‰' if has_importance else 'âŒ æ— '}")
            
        except Exception as e:
            results.append({
                'algorithm': algo,
                'status': 'âŒ å¤±è´¥',
                'mse': None,
                'mae': None,
                'r2': None,
                'has_feature_importance': False,
                'error': str(e)
            })
            print(f"  âŒ {algo} - è®­ç»ƒå¤±è´¥: {str(e)}")
        
        print()
    
    # è¾“å‡ºæ€»ç»“æŠ¥å‘Š
    print("=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    successful = [r for r in results if r['status'] == 'âœ… æˆåŠŸ']
    failed = [r for r in results if r['status'] == 'âŒ å¤±è´¥']
    
    print(f"âœ… æˆåŠŸ: {len(successful)}/{len(algorithms)}")
    print(f"âŒ å¤±è´¥: {len(failed)}/{len(algorithms)}")
    
    if successful:
        print("\nğŸ“Š æˆåŠŸç®—æ³•çš„æ€§èƒ½ç»Ÿè®¡:")
        successful_df = pd.DataFrame(successful)
        print(f"å¹³å‡ MSE: {successful_df['mse'].mean():.4f}")
        print(f"å¹³å‡ MAE: {successful_df['mae'].mean():.4f}")
        print(f"å¹³å‡ RÂ²: {successful_df['r2'].mean():.4f}")
        print(f"æ”¯æŒç‰¹å¾é‡è¦æ€§çš„ç®—æ³•: {successful_df['has_feature_importance'].sum()}/{len(successful)}")
    
    if failed:
        print("\nâŒ å¤±è´¥çš„ç®—æ³•:")
        for fail in failed:
            print(f"  - {fail['algorithm']}: {fail['error']}")
    
    # æŒ‰æ€§èƒ½æ’åº
    if successful:
        print("\nğŸ† æ€§èƒ½æ’å (æŒ‰RÂ²):")
        sorted_results = sorted(successful, key=lambda x: x['r2'], reverse=True)
        for i, result in enumerate(sorted_results[:5], 1):
            print(f"  {i}. {result['algorithm']}: RÂ² = {result['r2']:.4f}")
    
    return results

def test_model_trainer_initialization():
    """æµ‹è¯•ModelTrainerç±»çš„åˆå§‹åŒ–"""
    print("ğŸ§ª æµ‹è¯•ModelTraineråˆå§‹åŒ–...")
    
    test_cases = [
        ('lightgbm', 'LightGBMæ¢¯åº¦æå‡æ ‘'),
        ('xgboost', 'XGBoostæ¢¯åº¦æå‡æ ‘'),
        ('linear', 'çº¿æ€§å›å½’'),
        ('ridge', 'å²­å›å½’'),
        ('lasso', 'Lassoå›å½’'),
        ('elasticnet', 'å¼¹æ€§ç½‘ç»œ'),
        ('logistic', 'é€»è¾‘å›å½’'),
        ('random_forest', 'éšæœºæ£®æ—'),
        ('gradient_boosting', 'æ¢¯åº¦æå‡æ ‘'),
        ('decision_tree', 'å†³ç­–æ ‘'),
        ('catboost', 'CatBoost'),
        ('svm', 'æ”¯æŒå‘é‡æœº'),
        ('svr', 'æ”¯æŒå‘é‡å›å½’'),
        ('naive_bayes', 'æœ´ç´ è´å¶æ–¯'),
        ('knn', 'K-æœ€è¿‘é‚»')
    ]
    
    for model_type, description in test_cases:
        try:
            trainer = ModelTrainer(model_type=model_type)
            print(f"  âœ… {model_type} - {description} - åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ {model_type} - {description} - åˆå§‹åŒ–å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    print("ğŸ¤– æœºå™¨å­¦ä¹ ç®—æ³•åŠŸèƒ½éªŒè¯")
    print("=" * 60)
    
    # æµ‹è¯•ModelTraineråˆå§‹åŒ–
    test_model_trainer_initialization()
    print()
    
    # æµ‹è¯•æ‰€æœ‰ç®—æ³•è®­ç»ƒåŠŸèƒ½
    results = test_ml_algorithms()
    
    # æ€»ç»“
    successful_count = len([r for r in results if r['status'] == 'âœ… æˆåŠŸ'])
    total_count = len(results)
    
    print("\n" + "=" * 60)
    if successful_count == total_count:
        print("ğŸ‰ æ‰€æœ‰ç®—æ³•æµ‹è¯•é€šè¿‡ï¼æœºå™¨å­¦ä¹ è®­ç»ƒåŠŸèƒ½æ­£å¸¸ã€‚")
    else:
        print(f"âš ï¸  {successful_count}/{total_count} ä¸ªç®—æ³•æµ‹è¯•é€šè¿‡ã€‚")
        print("å»ºè®®æ£€æŸ¥å¤±è´¥çš„ç®—æ³•æ˜¯å¦éœ€è¦é¢å¤–çš„ä¾èµ–æˆ–å‚æ•°è°ƒæ•´ã€‚")