"""
MCPå·¥å…·å¤„ç†å‡½æ•°
åŒ…å«æ‰€æœ‰å·¥å…·çš„ä¸šåŠ¡é€»è¾‘å’Œé”™è¯¯å¤„ç†
"""
import logging
import time

from typing import Any, Dict, List
from mcp import types
import json
import pandas as pd

from .errors import (
    MCPError,
    validate_data_id,
    validate_factor_name,
    validate_required_columns,
    validate_data_length,
    validate_window_size,
    validate_period,
    validate_file_path
)
from .utils import serialize_response, convert_to_serializable
from ..logger import get_logger
from ..utils import get_progress_bar

def _convert_index_to_string(data_dict: dict) -> dict:
    """å°†å­—å…¸ä¸­çš„å…ƒç»„ç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼"""
    if not isinstance(data_dict, dict):
        return data_dict
    
    converted = {}
    for key, value in data_dict.items():
        if isinstance(key, tuple):
            # å°†å…ƒç»„ç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚ "(2023-01-01, 000001.SZ)"
            converted[str(key)] = value
        else:
            converted[str(key)] = value
    return converted

from quantanalyzer.data import DataLoader
from quantanalyzer.data.processor import (
    ProcessInf, CSZFillna, CSZScoreNorm, ZScoreNorm,
    RobustZScoreNorm, CSRankNorm, MinMaxNorm, ProcessorChain
)
from quantanalyzer.factor import FactorLibrary, FactorEvaluator, Alpha158Generator
# æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç§»é™¤ï¼Œä¸“æ³¨äºä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•
# from quantanalyzer.model.deep_models import LSTMModel, GRUModel, TransformerModel
# å…¨å±€å­˜å‚¨
data_store = {}
factor_store = {}
model_store = {}
processor_store = {}

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger = get_logger(__name__)

async def handle_preprocess_data(args: Dict[str, Any]) -> List[types.TextContent]:
    """æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—"""
    start_time = time.time()
    file_path = args["file_path"]
    data_id = args["data_id"]
    auto_clean = args.get("auto_clean", True)  # é»˜è®¤å¼€å¯è‡ªåŠ¨æ¸…æ´—
    export_path = args.get("export_path", None)  # å¯¼å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    logger.info(f"å¼€å§‹é¢„å¤„ç†æ•°æ®: data_id={data_id}, file_path={file_path}")
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„
    error = validate_file_path(file_path)
    if error:
        logger.error(f"æ–‡ä»¶è·¯å¾„éªŒè¯å¤±è´¥: {error}")
        return [types.TextContent(type="text", text=error)]
    
    try:
        loader = DataLoader()
        logger.debug(f"æ­£åœ¨åŠ è½½CSVæ–‡ä»¶: {file_path}")
        data = loader.load_from_csv(file_path)
        
        # éªŒè¯å¿…éœ€åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        logger.debug(f"éªŒè¯å¿…éœ€åˆ—: {required_cols}")
        error = validate_required_columns(data, required_cols, data_id)
        if error:
            logger.error(f"å¿…éœ€åˆ—éªŒè¯å¤±è´¥: {error}")
            return [types.TextContent(type="text", text=error)]
        
        # éªŒè¯æ•°æ®é‡ - æ”¯æŒå¿«é€Ÿæµ‹è¯•æ¨¡å¼
        min_data_length = 30  # é™ä½å¿«é€Ÿæµ‹è¯•é—¨æ§›
        logger.debug(f"éªŒè¯æ•°æ®é•¿åº¦ï¼Œæœ€å°è¦æ±‚: {min_data_length}")
        error = validate_data_length(data, min_length=min_data_length, data_id=data_id, operation_type="quick_test")
        if error:
            logger.error(f"æ•°æ®é•¿åº¦éªŒè¯å¤±è´¥: {error}")
            return [types.TextContent(type="text", text=error)]
        
        # è®°å½•åŸå§‹æ•°æ®è´¨é‡
        original_null_count = int(data.isna().sum().sum())
        original_null_rate = original_null_count / (data.shape[0] * data.shape[1]) if data.shape[0] * data.shape[1] > 0 else 0
        logger.info(f"åŸå§‹æ•°æ®è´¨é‡: null_count={original_null_count}, null_rate={original_null_rate:.4f}")
        
        # è‡ªåŠ¨æ•°æ®æ¸…æ´—
        cleaned = False
        cleaning_methods = []
        if auto_clean:
            try:
                # ProcessInf: å¤„ç†æ— ç©·å€¼
                logger.debug("åº”ç”¨ProcessInfå¤„ç†å™¨")
                process_inf = ProcessInf()
                data = process_inf(data)
                cleaning_methods.append("ProcessInf")
                
                # CSZFillna: æˆªé¢å¡«å……ç¼ºå¤±å€¼
                logger.debug("åº”ç”¨CSZFillnaå¤„ç†å™¨")
                csz_fillna = CSZFillna()
                data = csz_fillna(data)
                cleaning_methods.append("CSZFillna")
                
                cleaned = True
                logger.info(f"è‡ªåŠ¨æ•°æ®æ¸…æ´—å®Œæˆï¼Œä½¿ç”¨æ–¹æ³•: {cleaning_methods}")
                
                # è®°å½•æ¸…æ´—åæ•°æ®è´¨é‡
                cleaned_null_count = int(data.isna().sum().sum())
                cleaned_null_rate = cleaned_null_count / (data.shape[0] * data.shape[1]) if data.shape[0] * data.shape[1] > 0 else 0
                logger.info(f"æ¸…æ´—åæ•°æ®è´¨é‡: null_count={cleaned_null_count}, null_rate={cleaned_null_rate:.4f}")
                
            except Exception as e:
                logger.warning(f"è‡ªåŠ¨æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†
        
        # å­˜å‚¨åˆ°å…¨å±€å­˜å‚¨
        data_store[data_id] = data
        logger.info(f"æ•°æ®å·²å­˜å‚¨åˆ° data_store[{data_id}]ï¼Œå½¢çŠ¶: {data.shape}")
        
        # å¯¼å‡ºæ•°æ®ï¼ˆå¦‚æœæŒ‡å®šäº†å¯¼å‡ºè·¯å¾„ï¼‰
        export_info = ""
        if export_path:
            try:
                logger.debug(f"å¯¼å‡ºæ•°æ®åˆ°: {export_path}")
                data.to_csv(export_path, encoding='utf-8-sig')  # ä½¿ç”¨utf-8-sigç¼–ç æ”¯æŒä¸­æ–‡
                export_info = f"\nğŸ’¾ æ•°æ®å·²å¯¼å‡ºåˆ°: {export_path}"
            except Exception as e:
                logger.error(f"å¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {e}")
                export_info = f"\nâš ï¸ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}"
        
        # æ„å»ºå“åº”ä¿¡æ¯
        elapsed_time = time.time() - start_time
        response_text = (
            f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\n"
            f"ğŸ“ æ•°æ®ID: {data_id}\n"
            f"ğŸ“Š æ•°æ®å½¢çŠ¶: {data.shape[0]}è¡Œ Ã— {data.shape[1]}åˆ—\n"
            f"ğŸ“ˆ è‚¡ç¥¨æ•°é‡: {data.index.get_level_values('symbol').nunique()}\n"
            f"ğŸ“… æ—¶é—´èŒƒå›´: {data.index.get_level_values('datetime').min().strftime('%Y-%m-%d')} ~ {data.index.get_level_values('datetime').max().strftime('%Y-%m-%d')}\n"
            f"ğŸ§¼ æ•°æ®æ¸…æ´—: {'å·²å®Œæˆ' if cleaned else 'æœªæ‰§è¡Œ'}\n"
            f"   æ–¹æ³•: {', '.join(cleaning_methods) if cleaning_methods else 'æ— '}\n"
            f"ğŸ” åŸå§‹ç¼ºå¤±ç‡: {original_null_rate:.4f}\n"
            f"ğŸ” æ¸…æ´—åç¼ºå¤±ç‡: {cleaned_null_rate:.4f}\n"
            f"â±ï¸ å¤„ç†è€—æ—¶: {elapsed_time:.2f}ç§’"
            f"{export_info}"
        )
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        return [types.TextContent(type="text", text=response_text)]
        
    except Exception as e:
        logger.error(f"æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸçš„é”™è¯¯: {e}", exc_info=True)
        error_msg = MCPError.format_error(
            error_code=MCPError.PROCESSING_ERROR,
            message=f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}",
            suggestions=[
                "æ£€æŸ¥CSVæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®",
                "ç¡®ä¿åŒ…å«å¿…éœ€åˆ—: datetime, symbol, open, high, low, close, volume",
                "ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ä¸”æ–‡ä»¶å¯è®¿é—®"
            ]
        )
        return [types.TextContent(type="text", text=error_msg)]


async def handle_generate_alpha158(args: Dict[str, Any]) -> List[types.TextContent]:
    """ç”ŸæˆAlpha158å› å­"""
    start_time = time.time()
    data_id = args["data_id"]
    result_id = args["result_id"]
    kbar = args.get("kbar", True)
    price = args.get("price", True)
    volume = args.get("volume", True)
    rolling = args.get("rolling", True)
    rolling_windows = args.get("rolling_windows", [5, 10, 20, 30, 60])
    export_path = args.get("export_path", None)
    parallel = args.get("parallel", True)
    show_progress = args.get("show_progress", False)
    
    logger.info(f"å¼€å§‹ç”ŸæˆAlpha158å› å­: data_id={data_id}, result_id={result_id}")
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store, "generate_alpha158")
    if error:
        logger.error(f"æ•°æ®IDéªŒè¯å¤±è´¥: {error}")
        return [types.TextContent(type="text", text=error)]
    
    # è·å–æ•°æ®
    data = data_store[data_id]
    
    # éªŒè¯å¿…éœ€åˆ—
    required_cols = ['open', 'high', 'low', 'close', 'volume']
    error = validate_required_columns(data, required_cols, data_id)
    if error:
        logger.error(f"å¿…éœ€åˆ—éªŒè¯å¤±è´¥: {error}")
        return [types.TextContent(type="text", text=error)]
    
    try:
        # åˆ›å»ºAlpha158ç”Ÿæˆå™¨
        logger.debug("åˆ›å»ºAlpha158ç”Ÿæˆå™¨")
        generator = Alpha158Generator(data)
        
        # ç”Ÿæˆå› å­
        logger.debug("å¼€å§‹ç”Ÿæˆå› å­")
        factors = generator.generate_all(
            kbar=kbar,
            price=price,
            volume=volume,
            rolling=rolling,
            rolling_windows=rolling_windows,
            parallel=parallel,
            show_progress=show_progress
        )
        
        # å­˜å‚¨å› å­
        factor_store[result_id] = factors
        logger.info(f"å› å­å·²å­˜å‚¨åˆ° factor_store[{result_id}]ï¼Œå½¢çŠ¶: {factors.shape}")
        
        # å¯¼å‡ºå› å­ï¼ˆå¦‚æœæŒ‡å®šäº†å¯¼å‡ºè·¯å¾„ï¼‰
        export_info = ""
        if export_path:
            try:
                logger.debug(f"å¯¼å‡ºå› å­åˆ°: {export_path}")
                factors.to_csv(export_path, encoding='utf-8-sig')
                export_info = f"\nğŸ’¾ å› å­å·²å¯¼å‡ºåˆ°: {export_path}"
            except Exception as e:
                logger.error(f"å¯¼å‡ºå› å­æ—¶å‡ºé”™: {e}")
                export_info = f"\nâš ï¸ å› å­å¯¼å‡ºå¤±è´¥: {e}"
        
        # æ„å»ºå“åº”ä¿¡æ¯
        elapsed_time = time.time() - start_time
        response_text = (
            f"âœ… Alpha158å› å­ç”Ÿæˆå®Œæˆ\n"
            f"ğŸ“ è¾“å…¥æ•°æ®ID: {data_id}\n"
            f"ğŸ“ è¾“å‡ºå› å­ID: {result_id}\n"
            f"ğŸ“Š å› å­å½¢çŠ¶: {factors.shape[0]}è¡Œ Ã— {factors.shape[1]}åˆ—\n"
            f"ğŸ”¢ å› å­æ•°é‡: {factors.shape[1]}\n"
            f"âš™ï¸ ç”Ÿæˆé…ç½®:\n"
            f"   Kçº¿å› å­: {'å¼€å¯' if kbar else 'å…³é—­'}\n"
            f"   ä»·æ ¼å› å­: {'å¼€å¯' if price else 'å…³é—­'}\n"
            f"   æˆäº¤é‡å› å­: {'å¼€å¯' if volume else 'å…³é—­'}\n"
            f"   æ»šåŠ¨ç»Ÿè®¡å› å­: {'å¼€å¯' if rolling else 'å…³é—­'}\n"
            f"   çª—å£å¤§å°: {rolling_windows}\n"
            f"   å¹¶è¡Œå¤„ç†: {'å¼€å¯' if parallel else 'å…³é—­'}\n"
            f"â±ï¸ ç”Ÿæˆè€—æ—¶: {elapsed_time:.2f}ç§’"
            f"{export_info}"
        )
        
        logger.info(f"Alpha158å› å­ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        return [types.TextContent(type="text", text=response_text)]
        
    except Exception as e:
        logger.error(f"Alpha158å› å­ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸçš„é”™è¯¯: {e}", exc_info=True)
        error_msg = MCPError.format_error(
            error_code=MCPError.PROCESSING_ERROR,
            message=f"Alpha158å› å­ç”Ÿæˆå¤±è´¥: {str(e)}",
            suggestions=[
                "æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤æ•°æ®åŒ…å«å¿…éœ€åˆ—: open, high, low, close, volume",
                "å°è¯•å‡å°çª—å£å¤§å°æˆ–å…³é—­éƒ¨åˆ†å› å­ç±»å‹"
            ]
        )
        return [types.TextContent(type="text", text=error_msg)]